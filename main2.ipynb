{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "z7vYMic92jf2",
    "outputId": "6ec85d51-962f-4a1b-f086-09db1a63fe1e"
   },
   "outputs": [],
   "source": [
    "# # comment when run locally\n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "# %cd /gdrive/My\\ Drive/similar_faces/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rVdqfGug2lCi",
    "outputId": "4734a6db-6acb-45ee-ddf0-e8bd46e030dd"
   },
   "outputs": [],
   "source": [
    "# # uncomment when run locally\n",
    "# !rm -rf /data_celeba\n",
    "# !mkdir /data_celeba\n",
    "# %cp celeba_identity.txt /data_celeba\n",
    "# %cp img_align_celeba.zip /data_celeba\n",
    "# %cd /data_celeba\n",
    "# !unzip -q img_align_celeba.zip\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t50Sdd02BrX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G9YHmdxpRybo",
    "outputId": "057dd27f-9b9e-4487-ca29-27da2a1012b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda unavailable\n"
     ]
    }
   ],
   "source": [
    "sns.set()\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Cuda device: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('Cuda unavailable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IqxGOip22Bra"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT1R3z1J2Brb"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "train_test_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6oJAzSb2Brd"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarFaceDatasetOnline(Dataset):\n",
    "    def __init__(self, num_image_per_identity=4):\n",
    "        with open('celeba_identity.txt') as f:\n",
    "            filename_identity = [x.split() for x in f.readlines()]\n",
    "        identity_filenames_dict = defaultdict(list)\n",
    "        for i in filename_identity:\n",
    "            identity_filenames_dict[int(i[1])].append(i[0])\n",
    "        self.identity_filenames_list = sorted(list(identity_filenames_dict.items()), key=lambda x: x[0])\n",
    "        self.identity_filenames_list = list(map(lambda x: x[1], self.identity_filenames_list))\n",
    "        num_identities = len(self.identity_filenames_list)\n",
    "        self.identity_filenames_list = self.identity_filenames_list[:int(num_identities * train_test_split)]\n",
    "        self.identity_filenames_list = list(filter(lambda x: len(x) >= num_image_per_identity, self.identity_filenames_list))\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.num_image_per_identity = num_image_per_identity\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.identity_filenames_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filenames = random.sample(self.identity_filenames_list[index], self.num_image_per_identity)\n",
    "        images = torch.stack([self.transform(Image.open(f'img_align_celeba/{x}')) for x in filenames])\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGCXWV9f2Brf"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ib3aoGPK2Brg"
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "            \n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, out_channels=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], 2)\n",
    "        self.layer4 = self._make_layer(block, 128, layers[3], 2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(128, out_channels)\n",
    "        \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_net = ResNet(ResidualBlock, [2, 2, 2, 2], out_channels=10)\n",
    "        \n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n",
    "        \n",
    "    def forward(self, a, p, n):\n",
    "        a_out = self.embedding_net(a)\n",
    "        p_out = self.embedding_net(p)\n",
    "        n_out = self.embedding_net(n)\n",
    "        return (a_out, p_out, n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJctWy0d2Brh"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4320 triplets per batch\n"
     ]
    }
   ],
   "source": [
    "num_identity = 10 # number of identities\n",
    "num_image_per_identity = 4\n",
    "online_train_dataset = SimilarFaceDatasetOnline(num_image_per_identity=num_image_per_identity)\n",
    "online_train_loader = DataLoader(online_train_dataset, batch_size=num_identity, shuffle=True)\n",
    "triplet_indices = []\n",
    "for anchor_identity in range(num_identity):\n",
    "    for anchor_idx in range(num_image_per_identity):\n",
    "        for positive_idx in range(num_image_per_identity):\n",
    "            if anchor_idx != positive_idx:\n",
    "                for negative_identity in range(num_identity):\n",
    "                    if anchor_identity != negative_identity:\n",
    "                        for negative_idx in range(num_image_per_identity):\n",
    "                            triplet_indices.append((anchor_identity, negative_identity, anchor_idx, positive_idx, negative_idx))\n",
    "print(f'{len(triplet_indices)} triplets per batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKWdq7hC2Brl"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQ1k-2Jo2Brm"
   },
   "outputs": [],
   "source": [
    "# # Load checkpoint\n",
    "# checkpoint = torch.load('checkpoint_online', map_location=torch.device('cpu'))\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_triplet_loss(anchor, positive, negative):\n",
    "    \"\"\"Consider only\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyIRdr252Bro"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(online_train_loader):\n",
    "        batch = batch.reshape((-1, 3, 218, 178))\n",
    "        embeddings = model.get_embedding(batch)\n",
    "        embeddings = embeddings.reshape((num_identity, num_image_per_identity, -1))\n",
    "        anchor_embeddings = []\n",
    "        positive_embeddings = []\n",
    "        negative_embeddings = []\n",
    "        for i, j, k, l, m in triplet_indices:\n",
    "            anchor_embeddings.append(embeddings[i, k])\n",
    "            positive_embeddings.append(embeddings[i, l])\n",
    "            negative_embeddings.append(embeddings[j, m])\n",
    "        anchor_embeddings = torch.stack(anchor_embeddings)\n",
    "        positive_embeddings = torch.stack(positive_embeddings)\n",
    "        negative_embeddings = torch.stack(negative_embeddings)\n",
    "        \n",
    "        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5CUKOdilGtn"
   },
   "source": [
    "## Test on custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xzk5zP01gWBJ",
    "outputId": "a1a60a32-21a2-4ec1-cb37-4c64fe0437b9"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "count = 22\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((217, 178)),\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x[:3]\n",
    "])\n",
    "\n",
    "images = torch.empty(220, 3, 217, 178)\n",
    "j = 0\n",
    "for i in range(count):\n",
    "    pathname = os.path.join('./data', f'c{i+1}')\n",
    "    for filename in glob.glob(f'{pathname}/*'):\n",
    "        images[j] = transform(Image.open(filename))\n",
    "        j += 1\n",
    "    print(f'{i} ', end='')\n",
    "\n",
    "embeddings = model.get_embedding(images).detach().numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "df = pd.DataFrame()\n",
    "df['pca-1'] = pca_result[:, 0]\n",
    "df['pca-2'] = pca_result[:, 1]\n",
    "df['label'] = [i for i in range(count) for j in range(10)]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x='pca-1', \n",
    "    y='pca-2',\n",
    "    hue='label',\n",
    "    palette=sns.color_palette(\"hls\", count),\n",
    "    data=df\n",
    ")\n",
    "print(f'\\nExplained variance vatio: {pca.explained_variance_ratio_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_face_index = 200\n",
    "k = 10\n",
    "l2_distances = ((embeddings[target_face_index] - embeddings) ** 2).sum(axis=1)\n",
    "closest_k_face_indices = np.argsort(l2_distances)[:k]\n",
    "closest_k_faces = images[closest_k_face_indices]\n",
    "print(l2_distances[closest_k_face_indices])\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(np.transpose(images[target_face_index], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(np.transpose(make_grid(closest_k_faces, 5), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on celeba data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "count = 22\n",
    "with open('celeba_identity.txt') as f:\n",
    "    filename_identity = [x.split() for x in f.readlines()]\n",
    "    identity_filenames_dict = defaultdict(list)\n",
    "    for i in filename_identity:\n",
    "        identity_filenames_dict[int(i[1])].append(i[0])\n",
    "    identity_filenames_list = []\n",
    "    for i in range(len(identity_filenames_dict)):\n",
    "        tmp = []\n",
    "        for filename in identity_filenames_dict[i+1]:\n",
    "            tmp.append(filename)\n",
    "        if len(tmp) >= 10:\n",
    "            identity_filenames_list.append(tmp[:10])\n",
    "    mask = random.sample(range(len(identity_filenames_list)), count)\n",
    "    identity_filenames_list_ = []\n",
    "    for i in mask:\n",
    "        identity_filenames_list_.append(identity_filenames_list[i])    \n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "images = []\n",
    "for i in range(count):\n",
    "    tmp = []\n",
    "    for j in range(10):\n",
    "        filename = identity_filenames_list[i][j]\n",
    "        images.append(transform(Image.open(f'./img_align_celeba/{filename}')))\n",
    "images = torch.stack(images)\n",
    "\n",
    "embeddings = model.get_embedding(images)\n",
    "embeddings = embeddings.detach().numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "df = pd.DataFrame()\n",
    "df['pca-1'] = pca_result[:, 0]\n",
    "df['pca-2'] = pca_result[:, 1]\n",
    "df['label'] = [i for i in range(count) for j in range(10)]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x='pca-1', \n",
    "    y='pca-2',\n",
    "    hue='label',\n",
    "    palette=sns.color_palette(\"hls\", count),\n",
    "    data=df\n",
    ")\n",
    "print(f'\\nExplained variance vatio: {pca.explained_variance_ratio_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_face_index = 144\n",
    "k = 3\n",
    "l2_distances = ((embeddings[target_face_index] - embeddings) ** 2).sum(axis=1)\n",
    "closest_k_face_indices = np.argsort(l2_distances)[:k]\n",
    "closest_k_faces = images[closest_k_face_indices]\n",
    "print(l2_distances[closest_k_face_indices])\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(np.transpose(images[target_face_index], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(np.transpose(make_grid(closest_k_faces, 5), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
